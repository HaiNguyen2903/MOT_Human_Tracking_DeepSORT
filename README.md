# Running Tutorial

### Generate VTX data
Trong file **generate_data/combine_train_data.py**:
Thay **root_frames_dir** và **root_labels_dir** bằng folder có format:

```bash
root
| 
|___ VID_NAME_1
|       |___ frame_xxxxxx.jpg
|       |___ ...
|___ VID_NAME_2
        |___ frame_xxxxxx.jpg
        |___ ...
```

```bash
root
| 
|___ VID_NAME_1
|       |___ frame_xxxxxx.txt
|       |___ ...
|___ VID_NAME_2
        |___ frame_xxxxxx.txt
        |___ ...
```

Trong thư mục **generate_data**:

```bash
sh create_data_tree.sh
```

Data sau khi generate có dạng:
```bash
root
| 
|___ images
|       |___ train
|       |       |___ frame_xxxxxx.jpg
|       |       |___ ...
|       |___ val
|               |___ frame_xxxxxx.jpg
|               |___ ...       
|        
|___ labels
        |___ train
        |       |___ frame_xxxxxx.txt
        |       |___ ...
        |___ val
                |___ frame_xxxxxx.txt
                |___ ...      
```


### Training Custom Data
**Follow** [Training Custom Data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)

**Lưu ý:**

0. Tạo data folder đúng format folder do YoloV5 check label path theo image path.
1. Trong file **data.yaml**, thay đổi nc = 1, names = ['person']. Thay đổi **train** và **val** bằng absolute path thay vì relative path với **path**
2. Nên tạo file data config yaml trong yolov5/data/
3. Training với checkpoint **crowdhuman_yolov5** cần xoá phần Optimizer, nếu không sẽ bị conflict trong quá trình training
4. File [hyp.scratch.yaml](https://github.com/ultralytics/yolov5/issues/607) trong trường hợp repo không có (để trong folder yolov5/data/)
5. Training script

```bash
python train.py --data {data_yaml_file_config} --epochs {num_epochs} --batch {batches} --weights {weights path} --cfg {model config path}
```
Nếu sử dụng checkpoint **crowdhuman_yolov5** có thể sử dụng config file của yolov5m trong yolov5/models/yolov5m.yaml

Kết quả sau khi training được lưu trong /yolov5/runs/train/exp{x}. 

**Checkpoint model đã finetune và training 30 epoch trên VTX DATA: [Checkpoint](https://wandb.ai/hainguyen/YOLOv5/artifacts/model/run_3gqwg2vr_model/ebe1245d78646d98df91/files)**

### Evaluate Detection Module (YOLOV5)
```bash
python test.py --data {data_yaml_file_config} --weights {weights_path} --save-txt --save-conf
```
Trong đó file **data config yaml** để **train path** và **val path** là absolute path đến thư mục images của test data (model test toàn bộ thư mục images)

File label sau khi evaluate được lưu trong /yolov5/runs/test/exp{x}

Kết quả evaluate **Finetune Model** trên VTX DATA sau khi train 30 epochs và **Model** pretrained trên CrowdHman Dataset: **[Evaluation Results](https://docs.google.com/spreadsheets/d/1BOKNfHO-Ar7BzfpYRyFjux44B-I44XICpk7thhqd3MY/edit?fbclid=IwAR1GgUpXwZGpfFvW5TSdUTRWC09U4OIxLK2ajcDB218c0WngXt9ypyqVNhc#gid=0)**



### Inferrence Detection Module (YOLOV5)
```bash
python detect.py --source {data_source_path} --weights {weights_path} --save-txt --save-conf
```
Trong đó source có thể path đến 1 ảnh hoặc cả folder ảnh

Kết quả Inference được lưu trong yolov5/runs/detect/exp{x}



# Yolov5 + Deep Sort with PyTorch





<div align="center">
<p>
<img src="MOT16_eval/track_pedestrians.gif" width="400"/> <img src="MOT16_eval/track_all.gif" width="400"/> 
</p>
<br>
<div>
<a href="https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/actions"><img src="https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/workflows/CI%20CPU%20testing/badge.svg" alt="CI CPU testing"></a>
<br>  
<a href="https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
 
</div>

</div>


## Introduction

This repository contains a two-stage-tracker. The detections generated by [YOLOv5](https://github.com/ultralytics/yolov5), a family of object detection architectures and models pretrained on the COCO dataset, are passed to a [Deep Sort algorithm](https://github.com/ZQPei/deep_sort_pytorch) which tracks the objects. It can track any object that your Yolov5 model was trained to detect.


## Tutorials

* [Yolov5 training on Custom Data (link to external repository)](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)&nbsp;
* [Deep Sort deep descriptor training (link to external repository)](https://github.com/ZQPei/deep_sort_pytorch#training-the-re-id-model)&nbsp;
* [Yolov5 deep_sort pytorch evaluation](https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/wiki/Evaluation)&nbsp;



## Before you run the tracker

1. Clone the repository recursively:

`git clone --recurse-submodules https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch.git`

If you already cloned and forgot to use `--recurse-submodules` you can run `git submodule update --init`

2. Make sure that you fulfill all the requirements: Python 3.8 or later with all [requirements.txt](https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/blob/master/requirements.txt) dependencies installed, including torch>=1.7. To install, run:

`pip install -r requirements.txt`


## Tracking sources

Tracking can be run on most video formats

```bash
python3 track.py --source ... --show-vid  # show live inference results as well
```

- Video:  `--source file.mp4`
- Webcam:  `--source 0`
- RTSP stream:  `--source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa`
- HTTP stream:  `--source http://wmccpinetop.axiscam.net/mjpg/video.mjpg`


## Select a Yolov5 family model

There is a clear trade-off between model inference speed and accuracy. In order to make it possible to fulfill your inference speed/accuracy needs
you can select a Yolov5 family model for automatic download

```bash
python3 track.py --source 0 --yolo_weights yolov5s.pt --img 640  # smallest yolov5 family model
```

```bash
python3 track.py --source 0 --yolo_weights yolov5x6.pt --img 1280  # largest yolov5 family model
```


## Filter tracked classes

By default the tracker tracks all MS COCO classes.

If you only want to track persons I recommend you to get [these weights](https://drive.google.com/file/d/1gglIwqxaH2iTvy6lZlXuAcMpd_U0GCUb/view?usp=sharing) for increased performance

```bash
python3 track.py --source 0 --yolo_weights yolov5/weights/crowdhuman_yolov5m.pt --classes 0  # tracks persons, only
```

If you want to track a subset of the MS COCO classes, add their corresponding index after the classes flag

```bash
python3 track.py --source 0 --yolo_weights yolov5s.pt --classes 16 17  # tracks cats and dogs, only
```

[Here](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) is a list of all the possible objects that a Yolov5 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero.


## MOT compliant results

Can be saved to `inference/output` by 

```bash
python3 track.py --source ... --save-txt
```


## Cite

If you find this project useful in your research, please consider cite:

```latex
@misc{yolov5deepsort2020,
    title={Real-time multi-object tracker using YOLOv5 and deep sort},
    author={Mikel Broström},
    howpublished = {\url{https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch}},
    year={2020}
}
```


## Other information

For more detailed information about the algorithms and their corresponding lisences used in this project access their official github implementations.


# draw pred and gt boxes:
 File track.py:
- Comment line 298
- Remove opt.mode in line 216, 234
